# -*- coding: utf-8 -*-
"""k-armed bandit.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1K40x4eJrrHVLxmb6sAfbfq6qW9FGuG1a
"""

import numpy as np
import matplotlib.pyplot as plt
import random

class Bandit:
  def __init__(self):
    #Creates 10 arms, each with a random rewrd value from a normal distibution with a standard deviation of 1 amd a mean of 0
    self.arm_values = np.random.normal(0, 1, 10)
    #To keep track of how many times each arm was pulled
    self.k = np.zeros(10)
    #To keep track of the estimated arm values
    self.est_values = np.zeros(10)
    #To keep track of the total regret
    self.regret = 0

  def get_reward(self, action):
    #Gets the value of the reward then adds noise it, which is also normally distributed
    reward = self.arm_values[action]
    return reward
    

  def choose_eps_greedy(self, epsilon):
    #Generates a random number between 0 and 1
    rand_num = random.random()
    if epsilon > rand_num:
      #If the random number > epsilon return a random integer between 1 and 10 corresponding to the arm to be pulled
      return np.random.randint(10)
    else:
      #Otherwise return the number of the arm with the highest estimated value
      return np.argmax(self.est_values)
  
  def choose_softmax(self, temperature):
    #Creates the softmax distribution, which is proportional to the estimated rewards of each arm
    pi = np.exp(self.est_values / temperature) / np.sum(np.exp(self.est_values / temperature))
    #Picks each arm using this softmax distribution
    return np.random.choice(range(10),p=pi)

  def choose_ucb1(self, current_pull_num):
    #Implementation of the Upper Confidence Bound 1 algorithm
    upper_bounds = []
    for i in range(10):
      upper_bounds.append(self.est_values[i] + np.sqrt(2*np.log(current_pull_num) / (self.k[i])))

    return np.argmax(upper_bounds)

  def update_est(self, action, reward):
    #Increment the number of times the arm was pulled by 1
    self.k[action] += 1
    #Updates the estimated values for the most recently pulled arm
    alpha = 1./self.k[action]
    self.est_values[action] += alpha * (reward - self.est_values[action])

def eps_greedy_experiment(bandit, num_pulls, epsilon):
  history = []
  history = []
  for i in range(num_pulls):
    if i == 1:
      #Pull each arm once at the start
      for i in range(10):
        R = bandit.get_reward(i)
        #Updates the total regret
        bandit.regret += (max(bandit.arm_values) - R)
        #Updates the estimated value of the pulled arm based on the reward
        bandit.update_est(action, R)
        #Keeps a record of the total regret
        history.append(bandit.regret)
    else:
      #Decides which arm to pull
      action = bandit.choose_eps_greedy(i)
      #Gets the reward of the pulled arm
      R = bandit.get_reward(action)
      #Updates the total regret
      bandit.regret += (max(bandit.arm_values) - R)
      #Updates the estimated value of the pulled arm based on the reward
      bandit.update_est(action, R)
      #Keeps a record of the total regret
      history.append(bandit.regret)
  return np.array(history)

def softmax_experiment(bandit, num_pulls, temperature):
  history = []
  for i in range(num_pulls):
    if i == 1:
      #Pull each arm once at the start
      for i in range(10):
        R = bandit.get_reward(i)
        #Updates the total regret
        bandit.regret += (max(bandit.arm_values) - R)
        #Updates the estimated value of the pulled arm based on the reward
        bandit.update_est(action, R)
        #Keeps a record of the total regret
        history.append(bandit.regret)
    else:
      #Decides which arm to pull
      action = bandit.choose_softmax(temperature)
      #Gets the reward of the pulled arm
      R = bandit.get_reward(action)
      #Updates the total regret
      bandit.regret += (max(bandit.arm_values) - R)
      #Updates the estimated value of the pulled arm based on the reward
      bandit.update_est(action, R)
      #Keeps a record of the total regret
      history.append(bandit.regret)
  return np.array(history)

def ucb1_experiment(bandit, num_pulls):
  history = []
  for i in range(num_pulls):
    if i == 1:
      #Pull each arm once at the start
      for i in range(10):
        R = bandit.get_reward(i)
        #Updates the total regret
        bandit.regret += (max(bandit.arm_values) - R)
        #Updates the estimated value of the pulled arm based on the reward
        bandit.update_est(action, R)
        #Keeps a record of the total regret
        history.append(bandit.regret)
    else:
      #Decides which arm to pull
      action = bandit.choose_ucb1(i)
      #Gets the reward of the pulled arm
      R = bandit.get_reward(action)
      #Updates the total regret
      bandit.regret += (max(bandit.arm_values) - R)
      #Updates the estimated value of the pulled arm based on the reward
      bandit.update_est(action, R)
      #Keeps a record of the total regret
      history.append(bandit.regret)
  return np.array(history)


num_pulls = 1000
print(Bandit().arm_values)
eps_greedy = eps_greedy_experiment(Bandit(), num_pulls, 0.1)
softmax = softmax_experiment(Bandit(), num_pulls, 0.1)
UCB1 = ucb1_experiment(Bandit(), num_pulls)

plt.plot(eps_greedy, label="Epsilon Greedy, eps = 0.1")
plt.plot(softmax, label="Softmax, temp = 0.1")
plt.plot(UCB1, label="Upper Confidence Bound")

plt.xlabel("Number of Pulls")
plt.ylabel("Total Regret")
plt.legend()
plt.show()
